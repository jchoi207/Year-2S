## Lecture 1 
- AI was coined in 1956 with the attempt to reproduce human intelligence with machines 

### Symbolic Approach 
- Early AI 
- Model the knowledge of an adult 
- Construct well-defined discrete symbols (like words) 
- Used to prove math theorems 
- Too abstract to generalize to the real-world

### Connectionist Approach
- Dominated AI since 2012 
- Simulate how a baby learns (start from an untrained brain)
- Feed it with observations (data)
- Let it learn from observations 
- Requires large scale data and compute 
    - GPT is not new, just scaled up, trained on the entire internet 


### Subfields of Artificial Intelligence
- Machine Learning (ML)
- Computer Vision (CV)
- Natural Language Processing (NLP)

### Machine Learning
- Enables comptuers to learn from data, rather than hardcoded 
- When public talks about AI, they are referring to ML
- ML itself is a very broad term, there are many methods/subfields:
    - Decision trees and Random Forests
    - K-Nearest Neighbors
    - Naive Bayes
    - Support Vector Machines
    - Gaussian Processes 
- However, we will be focusing on Neural Networks and Deep Learning


#### Why do we need ML
*e.g. How do we write a program to recognize a goat?*
- We could hardcode the rules, but in the real world, we will have some *counterexamples*
- Thus, we cannot formulate rules for all the conditions
- Simple images are high dimensional input spaces (e.g. 100x100 pixels = 10,000 dimensions), ML attempts to learn from simpler examples
    - In ML you don't write the program, you learn it

#### Formal Definition of ML
*"A computer program is said to learn from **experience E** with respect to some class of **tasks T** and **performance measures P**, if its performance at tasks in T, as measured by P, improves with experience E"*

### Deep Learning 
- Deep Learning is the latest version of Artificial Neural Networks (ANN) 
- Neural Networks were inspired by the brain 
- Initially we had Shallow Neural Networks, but they were not powerful enough
- By scaling them up, we get Deep Neural Networks with higher performance through lots of data, eventually they surpass human performance

#### Formal Definition of Deep Learning
- "Deep learning is a subset of machine learning that allows multiple levels of representation, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level."
(LeCun et al. 2015)

### Terminology Recap: 
- Artificial Intelligence (AI): is broad and poorly defined 
- Machine Learning (ML): computers learn by example from data, rather than being hardcoded
- Deep Learning (DL): A machine learning method that learns from data, using neural networks with many layers





### Successes and Caveats of Deep Learning
- **Sucessess*: Translation, Drug discoveries, speech recognition, image generation, alpha folding
- **Caveats** 
    - Interpretability, we are not able to understand why the model makes the decisions it does
    - Adversarial Examples: Small perturbations to the input that cause the model to make mistakes e.g. adding noise to a Dog will cause the model to classify it as a Cat
    - Correlation does not always mean causality. Deep Learning models can find patterns in the data (correlations) 
    - Bias: Models can be biased based on the data they are trained on. 
        - e.g. reading through CV of employees were accepted, training the models, the models rejected women, since majority were male. This does not mean that the model is wrong, but it is biased based on the data it was trained on.


### Types of Learning
*Supervised Learning:*
- Regression (real-valued or continuous value)
- Classification (categorical or labels)
- Requires data with ground-truth labels or outputs 

*Unsupervised Learning:*
- Self-supervised learning, semi-supervised learning 
- Requires observations without human annotations 

*Reinforcement Learning:*
- Sparse rewards from environment (e.g. winning or losing)
- Actions affect the environment (dynamic)


#### Supervised Learning
- Model learns to map an input to an outptu based on example **input-output pairs**
- Similar to a teacher guiding a student, but with many more examples

e.g. Regression Problem:
- Fitting a polynomial (regression) given some noisy sample data, we want to find the polynomial that generated the data
- In the real world, we never have the true function, but only the generated sample data
- We don't want to overfit (memorize) or underfit the data 
- But how would we know if we are overfitting or underfitting?
- In reality, the data is a polynomial of degree 3, but how would we know?

### Assumptions
- We use knowledge of the problem itself to seelct an appropriate modelling technique 
- Inductive bias (learning bias) is the set of assumptions that is used for modelling 
- So going back to the polynomial example, we need a way to quantify model performance 
- There are many error metrics, but we will used Mean Squared Error (MSE): computing the squared distance between the predicted and true values, and averaging them

### Training and Testing Data 
- More data means a better model, however, we shouldn't use all the data for training. 
- So we would like to test the model on unseen data, to see how well it generalizes
- Thus, we train different models with different degrees of polynomials. 

### Generalization, Overfitting, and Underfitting
- ML is game of balance 
**Bias-Variance Tradeoff**
- Greater model complexity leads to higher variance and chance of overfitting 
- Lower model complexity leads to higher bias and underfitting

![Bias-Variance Tradeoff](l1.png)
- Prediction and training data (green)
- Prediction and testing data (red)

- When over-fitting (too many parameters), the model is no longer generalizing, but is memorizing the training data, so naturally, the error would be low on the training data, but high on the testing data
- When underfitting, the model is **too simple to capture the underlying structure of the data**, so the error would be high on both the training and testing data

### Validation and Holdhout Data:
1. Training 
2. Validation: to decide which model to choose (e.g. choosing the polynomial degree)
3. Testing (holdout): to evaluate the model on unseen data

#### Sampling Training, validation and Holdout Data
- Sampling should be s.t. that the distribution of features within all three sets are the same
- For the ratio, it is common to use a 60-20-20 split for training, validation, and testing data respectively

