
#  Lecture 2
- Content: Neurons, Activation Function, Training an Artificial Neuron, Loss Functions, Gradient Descent, Neural Network Architecture

##  Neurons
###  Simplified Biological Neuron
- Dendrites: receive info from other neurons
- Cell body: consolidates information
- Axon: Passes info to other neurons
- Synapse: Connection between neurons

*Neurons fire on stimuli like: edges, lines, angles, movements, familiar faces, regardless of scale rotation and translation*

###  Artificial Neuron
- *Consider a house with a price of 1 M,   rooms =5, sqft = 2000, year = 1980, basement = 1* 
    - We represent the features as a 4D vector [x1, x2, x3, x4]
    - Each feature is multiplied by a weight, $w_1, w_2, w_3, w_4$ and summed up
    - Bias term is added to the sum 
    - Sum is passed through an activation function and outputs a value 

- $x_i$ is the input 
- $w_i$ is the weight for input $x_i$ that we learn 
- $b$ is a weight we learn with no input
- $f$ is the activation function that determines how output changes with the sum of all weighted inputs
- $y$ is the output of the neuron 
$$y = f\left(\sum_{i=1}^{n} w_i x_i + b\right)$$

We note that we can represent the weights and inputs as vectors: 
$$y = f(\mathbf{w}\cdot \mathbf{x}+b)$$
- Where $\mathbf{w}$ is an n-dimensional vector of weights, $\mathbf{x}$ is an n-dimensional vector of inputs, and $b$ is the bias term

##  Activation Functions
###  Linear Activation Function
- If activation function is linear, then 
$$y = \mathbf{w}\cdot \mathbf{x}+b$$
- This line separates the input space into two regions
- We learn the parameters $w$ and $b$ such that the line maximizes the correct separations
- The bias term shifts the line up or down, which may be necessary if the data is not separable from the origin
- With higher dimensions, the "line" becomes a hyperplane, so in our house example, we would have a line in 4d
###  Disadvantages of Linear Activation Function
- Most datasets are not linearly separable
- Non-linearity is essential
- Multiple layers with non-linear transformation help
- Note, there is no advantage from multiple lienar layers
$$ \mathbf{W}_2 \cdot \mathbf{W}_1 \cdot \mathbf{x} \cdot \mathbf{W}_3=  \mathbf{W} \cdot \mathbf{x}$$


![Linear Activation Function](img\l2_1.png)


###  Early Activation Functions
- 1943-70s: used a simple binary activation function
$$y = \text{sign}(\mathbf{w}\cdot \mathbf{x}+b) \{-1, 1\}$$
$$f(x) = \text{sign}(x)$$ 
- Or Heaviside step function
$$f(x) = \begin{cases} 0 & x < 0 \\ 1 & x \geq 0 \end{cases}$$
- This is called the decision boundary 
**Disadvantage**: The issue with these functions were that they were not differentiable, continuous, or smooth

###  Sigmoid Activation Function
- 2000s: sigmoid activation functions 
    - Easily differentiable, smooth, continuous 
    - Same range between 0 and 1 or -1 and 1
- Hyperbolic tangent function
$$f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \{-1, 1\}$$
- Logistic function
$$f(x) = \frac{1}{1+e^{-x}} \{0, 1\}$$
What was great about the Logistic function is that they can represent probabilities. e.g. 0.68 probability of being a cat

**Disadvantage**: saturated neurons "kill the gradient". gradients become very small very equickly away from x = 0

![Sigmoid Activation Function](img\l2_2.png)

###  Rectified Linear Unit (ReLU)
- Most popular activation function ReLU
$$\text{ReLU}(x) = (x)^+ = \max(0, x)$$
- Leaky ReLU: 
$$\text{Leaky ReLU}(x) = \begin{cases} x & x \geq 0 \\ \text{negative slope }x & \text{else} \end{cases}$$
- We realize that the negative slope is a parameter that we can learn, *just like a weight*, which is what parametric ReLU is
$$\text{PReLU}(x) = \begin{cases} x & x \geq 0 \\ a x & \text{else}\end{cases}$$

###  Continuous Approimations of ReLU
- SiLU (Sigmoid Linear Unit) or Swish
$$\text{SiLU}(x) = x \cdot \sigma(x) = \frac{x}{1+e^{-x}}$$
- SoftPlus
$$\text{SoftPlus}(x) = \frac{1}{\beta}\log(1+e^{\beta x})$$

###  Note:
- Always start with ReLU, it is cheap to compute, and very good performance in 99 \% of cases
$$ \text{ReLU}(x) = w x +b = \text{max}(0, w x + b)$$


##  Training Neural Networks 

###  Training Neuron
*How do we learn the weights and biases of a neural network?*
- We use our prediction error to decide how to update weights and biases
    - e.g. training data from historical house prices in Toronto
- We want our model to predict the price of the house to be as close as possible to the true price of historical data, s.t. we can use it to predict future house prices
- input: $\mathbf{x}$, predicted output: $y$, ground truth label: $t$, Neuron $M(\mathbf{w}, \mathbf{x})
1. Make a **prediction** for some input data $\mathbf{x}$ with a known correct output $t$
$$y = M(\mathbf{w}, \mathbf{x})$$
2. Compare the correct output with our predicted output to compute **loss**
$$E = \text{Loss}(y, t)$$
3. **Adjust the weights and bias** to make the prediction closer to the ground truth, i.e. minimize the error
4. **Repeat** until we have an acceptable level of error

We call step 1 **forward pass**, used for both training and inference
We call step 3 **backward pass** or **backpropagation** to update the weights and biases

##  Loss Function
- A **loss function** computes how bad predictions are
    - **Large loss**: the network's prediction differs from the true value
    - **Small loss**: the network's prediction is close to the true value
- It is important to track the loss value for training and validation data. We calculate the error over all **training examples**
    - On training data: loss decreases with time 
    - On validation data: loss decreases, then increases (overfitting) because the model is memorizing the training data
        - So *sweet spot** is when the validation loss is a minimum

###  $n$ Classes
- Suppose we want to train a linear neuron to differentiate images into three classes
- If we only have one neuron, then we can only distinguish between two classes positive and negative 
- **So if we want $n$ classes, we need $n$ neurons**

![Loss Function](img\l2_3.png)

- With our image, let's assume we break our image into a 2x2 matrix of pixels
- We flatten the matrix into a 1D vector of 4 elements
- The first row of the $\mathbf{W}$ matrix are the weights of the 1st neuron, and so on 
- The first element of the $\mathbf{b}$ bias vector is the bias of the 1st neuron, and so on
- Thus the result, is that we get a vector with the outputs of each neuron

_
- In the image, we see that our image of a cat (ground truth) is classified as a dog with a dog score of 437.9
- We cannot compare 437.9 to "cat", so we need a canonical representation, meaning we need to convert the scores to probabilities

#### Softmax Function
- Normalizes the **logits** (scores) into a categorical probability distribution over all possible classes: 
$$\text{Softmax}(x)_i = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}$$
- Note the softmax is good because it is relatively invariant to the scale of the input

- So the scores -96.8, 437.9, 61.95 are converted to probabilities: 0.01, 0.97, 0.02
- Note $0 \leq p_i \leq 1$ and $\sum_{i=1}^{n} p_i = 1$

![Softmax Function](img\l2_4.png)
#### One Hot Encoding
- Maps categories/classes (cat, dog, ship) to vector representations
- Cat: [1, 0, 0], Dog: [0, 1, 0], Ship: [0, 0, 1]
    - e.g. we are saying a cat is defined by the probability of cat being 1, and the probability of dog and ship being 0

![One Hot Encoding](img\l2_5.png)


#### Mean Squared Error (MSE)
- Mostly used for regression problems

$$\text{MSE} = \frac{1}{N} \sum_{n=1}^{N} (y_n - t_n)^2$$
- $N$ is the number of training samples
- $y_n$ is the predicted value
- $t_n$ is the true value
- So going to our example, our predictions were [0.01, 0.97, 0.02] and the true value was [1, 0, 0]:
$$\text{MSE} = (0.01 -1)^2 = 0.98$$

#### Cross Entropy Loss
- Mostly used for classification problems
$$\text{Cross Entropy Loss} = - \frac{1}{N}\sum_{n=1}^{N} \sum_{k=1}^{K} t_{n,k} \log(y_{n,k})$$
- We have -log of probability of the prediction and multiply it 
- $t_{n,k}$ multiplies the log of the probability of the ground truth class (since the other classes are 0)
- So going to our example, our predictions were [0.01, 0.97, 0.02] and the true value was [1, 0, 0]:
- So $- \log(0.01) = 2$ 
- Consider if the model gives us a probability of 1, then $-\log(1) = 0$, so the loss is 0
- Conversely, if prediction assigns a probability 0 to the true class, then $-\log(0) = \infty$, so the loss is infinite
- Usually log base 2, so the loss is in bits
- Note that the probability of labels must be normalized


#### Binary Cross Entropy Loss
- Only for 2 classes
- Bernoulli distribution
    - Two outcomes, so if you know the probability of one outcome, you know the probability of the other
    - We can plug this into CE
$$\text{Binary Cross Entropy Loss} = - \frac{1}{N}\sum_{n=1}^{N} t_n \log(y_n) + (1-t_n) \log(1-y_n)$$



## Forward-Pass with Error Calculations

- See lecture_2.py for code


## Gradient Descent
Consider a function like $f(x) = x^2 + 2$, the derivative is $f'(x) = 2x$
- We call this a derivative in a univariate function (one variable)

Consider a function like $f(x,y,z) = x^2 +2xy +zy$ 
- We compute the partial derivatives with respect to each variable, and we call this vector of derivatives as gradients: 
$$\nabla f = \begin{bmatrix} \frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \\ \frac{\partial f}{\partial z} \end{bmatrix}$$

Say we have 4 weights $w_1, w_2, w_3, w_4$ and our loss function:
- We compute the partial derivatives of the loss function with respect to each weight
- **So we get the contributions of each weight to the loss function**
- **We also get the direction to move the weights to maximize the loss function**
    - However, we want to minimize the loss, so we negate the gradient:
- Then, we update each of the weights by subtracting the gradient 

$$\mathbf{w} = \mathbf{w} - \alpha \nabla L = \begin{bmatrix} w_1 \\ w_2 \\ w_3 \\ w_4 \end{bmatrix} - \alpha \begin{bmatrix} \frac{\partial L}{\partial w_1} \\ \frac{\partial L}{\partial w_2} \\ \frac{\partial L}{\partial w_3} \\ \frac{\partial L}{\partial w_4} \end{bmatrix}$$

### Neural Network Single Layer Training 
- Vector of partial derivatives for all weights is the gradient
- Direction of the gradient is the direction in which the function increases most quickly 
- Magneitude of the gradient is the rate of increase

$$ w_{ji}^{t+1} = w_{ji}^{t} - \gamma \frac{\partial E}{\partial w_{ji}}$$
- Where $\gamma$ is the learning rate or step size

![Gradient Descent](img\l2_6.png)
- We keep updating the weights and repeat until we reach a minimum error


### Delta Rule for Single Weight/Training sample
- Let's assume our activation is sigmoid: $f(x) = \frac{1}{1+e^{-x}}$
- Let us assume we use MSE: $E = (y-t)^2$
- What is $\frac{\partial E}{\partial w_p}? 
- We can use the chain rule:
$$ \frac{\partial E}{\partial w_p} = \frac{\partial E}{\partial y} \frac{\partial y}{\partial a} \frac{\partial a}{\partial w_p} $$
- $\frac{dE}{dy} = 2(y-t)$
- $\frac{dy}{da} = (1-y)y$
- $\frac{da}{dw_p} = x_p$
- So $\frac{\partial E}{\partial w_p} = 2(y-t)(1-y)y x_p$

![Delta Rule](img\l2_7.png)



## Neural Network Architecture
### Multiple Layers are Important: XOR 
- Having a single decision boundary (a single NN layer) is not enough to solve many problems
- XOR is a classic function, which needs two decision boundaries
- So we need at least one hidden neural network layer (2 layers) to solve XOR
- In fact in the limit of a large number of neurons, we can approximate any function (universal approximation theorem)

- Note gradient descent = backpropagation + (chain rule )
- So multiple layers are able to separate linear and non-linear data

### Multiple Layers with Non-Linearity
![XOR](img\l2_8.png)
- Treating outputs of hidden layers as "high level features"


### Terminology: 
- Feedforward Neural Network: Data flows in one direction from one layer to a later layer 
- Fully Connected Neural Network (aka Multilayer Perceptron): if each neuron in one layer is connected to every neuron in the previous and next layer
- Number of layers: number of hidden layers + output layer
    - Input layers are not counted

### Architecture
- Architecture of a NN describes the neurons and their connectivity
- Graph, sequences, transformers, images etc.
